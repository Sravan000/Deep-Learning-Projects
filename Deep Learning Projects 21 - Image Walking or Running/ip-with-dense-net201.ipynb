{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import DenseNet201\n",
    "import tensorflow as tf, tensorflow.keras.backend as K\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.preprocessing.image import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers,optimizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import *\n",
    "\n",
    "\n",
    "''' directory path '''\n",
    "dataset_path = \"../input/walk-or-run\"\n",
    "\n",
    "''' train images path'''\n",
    "train_dir = os.path.join(dataset_path,'walk_or_run_train/train')\n",
    "\n",
    "''' test images path'''\n",
    "test_dir= os.path.join(dataset_path,'walk_or_run_test/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', 'run']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' total number of classes '''\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_start = 0.00001\n",
    "lr_max = 0.00005 \n",
    "lr_min = 0.00001\n",
    "lr_rampup_epochs = 5\n",
    "lr_sustain_epochs= 0\n",
    "lr_exp_decay = 0.8\n",
    "\n",
    "def learning_func(epoch):\n",
    "    if epoch < lr_rampup_epochs:\n",
    "        lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "    elif epoch < lr_rampup_epochs + lr_rampup_epochs:\n",
    "        lr = lr_max\n",
    "    else:\n",
    "        lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
    "    return lr\n",
    "    \n",
    "''' Callback '''\n",
    "lr_callback = keras.callbacks.LearningRateScheduler(learning_func(30), verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f696709d2eb6a10088cd2d7ae2ddbbc14796a25c"
   },
   "outputs": [],
   "source": [
    "''' Early Stopping '''\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_erasor(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(inp_img):\n",
    "        \n",
    "        ''' image height, width, and channel '''\n",
    "        img_height, img_width, img_channel = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return inp_img\n",
    "        \n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_height * img_width\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            \n",
    "            left = np.random.randint(0, img_width)\n",
    "            top = np.random.randint(0, img_height)\n",
    "\n",
    "            if left + w <= img_width and top + h <= img_height:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_channel))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        inp_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return inp_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "1e48fdc46842d550348eea153e906331b3db72ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 2 classes.\n",
      "Found 119 images belonging to 2 classes.\n",
      "Found 141 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "''' Data Augmentation '''\n",
    "train_ds = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, height_shift_range=0.1,\n",
    "                                   shear_range=0.1,zoom_range=0.1,horizontal_flip=True, vertical_flip=False,\n",
    "                                   validation_split=0.2, preprocessing_function = random_erasor(v_l=0, v_h=255))\n",
    "test_ds = ImageDataGenerator()\n",
    "\n",
    "''' flow from directory '''\n",
    "test_ds = test_ds.flow_from_directory(train_dir, target_size=(224, 224), batch_size=16, class_mode='categorical',seed=2019, \n",
    "                                      color_mode='rgb')\n",
    "\n",
    "val_ds = train.flow_from_directory(train_dir, target_size=(224, 224), batch_size=16, class_mode='categorical',\n",
    "                                   color_mode='rgb', subset='validation')\n",
    "\n",
    "test_ds = test_ds.flow_from_directory(test_dir, target_size=(224, 224), batch_size=16, class_mode='categorical', \n",
    "                                      color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model\n",
    "    with st.scope():\n",
    "        ''' DenseNet201 '''\n",
    "        model = keras.Sequential([DenseNet201(input_shape=(224,224, 3),include_top=False,weights='imagenet'),\n",
    "                keras.layers.GlobalAveragePooling2D(),\n",
    "                ''' classificaion layer ''' \n",
    "                keras.layers.Dense(2, activation='softmax')])\n",
    "        ''' compile the model '''\n",
    "        model.compile(optimizer='adam', oss = 'categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "74842112/74836368 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epochs = 600/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "38/38 [==============================] - 172s 5s/step - loss: 0.8290 - categorical_accuracy: 0.5817 - val_loss: 960.6427 - val_categorical_accuracy: 0.4184\n",
      "Epoch 2/30\n",
      "38/38 [==============================] - 21s 551ms/step - loss: 0.6439 - categorical_accuracy: 0.6400 - val_loss: 27.3813 - val_categorical_accuracy: 0.4113\n",
      "Epoch 3/30\n",
      "38/38 [==============================] - 21s 551ms/step - loss: 0.5777 - categorical_accuracy: 0.7083 - val_loss: 3.8149 - val_categorical_accuracy: 0.5816\n",
      "Epoch 4/30\n",
      "38/38 [==============================] - 22s 571ms/step - loss: 0.6036 - categorical_accuracy: 0.6733 - val_loss: 1.3337 - val_categorical_accuracy: 0.5957\n",
      "Epoch 5/30\n",
      "38/38 [==============================] - 21s 551ms/step - loss: 0.6252 - categorical_accuracy: 0.6700 - val_loss: 42.7393 - val_categorical_accuracy: 0.4184\n",
      "Epoch 6/30\n",
      "38/38 [==============================] - 21s 545ms/step - loss: 0.6647 - categorical_accuracy: 0.6583 - val_loss: 25.2281 - val_categorical_accuracy: 0.5887\n",
      "Epoch 7/30\n",
      "38/38 [==============================] - 22s 572ms/step - loss: 0.5984 - categorical_accuracy: 0.6983 - val_loss: 0.9624 - val_categorical_accuracy: 0.5816\n",
      "Epoch 8/30\n",
      "38/38 [==============================] - 21s 549ms/step - loss: 0.5897 - categorical_accuracy: 0.6883 - val_loss: 1.1011 - val_categorical_accuracy: 0.6241\n",
      "Epoch 9/30\n",
      "38/38 [==============================] - 21s 551ms/step - loss: 0.6271 - categorical_accuracy: 0.6800 - val_loss: 1.1808 - val_categorical_accuracy: 0.6028\n",
      "Epoch 10/30\n",
      "38/38 [==============================] - 22s 574ms/step - loss: 0.5850 - categorical_accuracy: 0.7000 - val_loss: 0.5661 - val_categorical_accuracy: 0.6667\n",
      "Epoch 11/30\n",
      "38/38 [==============================] - 21s 550ms/step - loss: 0.5552 - categorical_accuracy: 0.7283 - val_loss: 0.6289 - val_categorical_accuracy: 0.6879\n",
      "Epoch 12/30\n",
      "38/38 [==============================] - 21s 552ms/step - loss: 0.6072 - categorical_accuracy: 0.6933 - val_loss: 30.6830 - val_categorical_accuracy: 0.5957\n",
      "Epoch 13/30\n",
      "38/38 [==============================] - 21s 557ms/step - loss: 0.6009 - categorical_accuracy: 0.6883 - val_loss: 1.5914 - val_categorical_accuracy: 0.6170\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 14/30\n",
      "38/38 [==============================] - 21s 546ms/step - loss: 0.5698 - categorical_accuracy: 0.7217 - val_loss: 1.2123 - val_categorical_accuracy: 0.6809\n",
      "Epoch 15/30\n",
      "38/38 [==============================] - 21s 559ms/step - loss: 0.5421 - categorical_accuracy: 0.7283 - val_loss: 0.3927 - val_categorical_accuracy: 0.6667\n",
      "Epoch 16/30\n",
      "38/38 [==============================] - 21s 557ms/step - loss: 0.5170 - categorical_accuracy: 0.7583 - val_loss: 0.8929 - val_categorical_accuracy: 0.7234\n",
      "Epoch 17/30\n",
      "38/38 [==============================] - 21s 547ms/step - loss: 0.5067 - categorical_accuracy: 0.7633 - val_loss: 0.4944 - val_categorical_accuracy: 0.6950\n",
      "Epoch 18/30\n",
      "38/38 [==============================] - 22s 567ms/step - loss: 0.4810 - categorical_accuracy: 0.7717 - val_loss: 0.5558 - val_categorical_accuracy: 0.6738\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 19/30\n",
      "38/38 [==============================] - 21s 547ms/step - loss: 0.4789 - categorical_accuracy: 0.7567 - val_loss: 2.0070 - val_categorical_accuracy: 0.6738\n",
      "Epoch 20/30\n",
      "38/38 [==============================] - 21s 546ms/step - loss: 0.4552 - categorical_accuracy: 0.7950 - val_loss: 0.4644 - val_categorical_accuracy: 0.7589\n",
      "Epoch 21/30\n",
      "38/38 [==============================] - 22s 567ms/step - loss: 0.4249 - categorical_accuracy: 0.8050 - val_loss: 0.4720 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 22/30\n",
      "38/38 [==============================] - 21s 542ms/step - loss: 0.3753 - categorical_accuracy: 0.8433 - val_loss: 0.5964 - val_categorical_accuracy: 0.7163\n",
      "Epoch 23/30\n",
      "38/38 [==============================] - 21s 547ms/step - loss: 0.3805 - categorical_accuracy: 0.8450 - val_loss: 0.7212 - val_categorical_accuracy: 0.6809\n",
      "Epoch 24/30\n",
      "38/38 [==============================] - 22s 570ms/step - loss: 0.3813 - categorical_accuracy: 0.8300 - val_loss: 0.4336 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 25/30\n",
      "38/38 [==============================] - 21s 551ms/step - loss: 0.3423 - categorical_accuracy: 0.8500 - val_loss: 0.3704 - val_categorical_accuracy: 0.7801\n",
      "Epoch 26/30\n",
      "38/38 [==============================] - 21s 549ms/step - loss: 0.3226 - categorical_accuracy: 0.8517 - val_loss: 0.7997 - val_categorical_accuracy: 0.7660\n",
      "Epoch 27/30\n",
      "38/38 [==============================] - 22s 572ms/step - loss: 0.3163 - categorical_accuracy: 0.8633 - val_loss: 0.5801 - val_categorical_accuracy: 0.7447\n",
      "Epoch 28/30\n",
      "38/38 [==============================] - 21s 542ms/step - loss: 0.2937 - categorical_accuracy: 0.8800 - val_loss: 0.7700 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 29/30\n",
      "38/38 [==============================] - 21s 553ms/step - loss: 0.2801 - categorical_accuracy: 0.8800 - val_loss: 0.7483 - val_categorical_accuracy: 0.8156\n",
      "Epoch 30/30\n",
      "38/38 [==============================] - 22s 570ms/step - loss: 0.3001 - categorical_accuracy: 0.8633 - val_loss: 0.7462 - val_categorical_accuracy: 0.8085\n"
     ]
    }
   ],
   "source": [
    "''' training '''\n",
    "history = model.fit_generator(train_ds, epochs = 30, validation_data=test_ds,\n",
    "            callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = 0.5, patience =3, \n",
    "                                                           min_lr=0.00001, verbose=1, mode='min'),early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' evaluating the model '''\n",
    "results = model.evaluate_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.30673107504844666 test acc: 0.8085106611251831\n"
     ]
    }
   ],
   "source": [
    "print(\"test_loss:\",results[0],\"test acc:\",results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
