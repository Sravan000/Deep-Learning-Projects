{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.preprocessing.image import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers,optimizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import *\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "#walk is 1, run is 0\n",
    "original_dataset_dir = \"../input/walk-or-run\"\n",
    "\n",
    "train_dir = os.path.join(original_dataset_dir,'walk_or_run_train/train')\n",
    "#'../input/walk_or_run_train/train'\n",
    "test_dir= os.path.join(original_dataset_dir,'walk_or_run_test/test')\n",
    "#'../input/walk_or_run_test/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', 'run']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_START = 0.00001\n",
    "LR_MAX = 0.00005 \n",
    "LR_MIN = 0.00001\n",
    "LR_RAMPUP_EPOCHS = 5\n",
    "LR_SUSTAIN_EPOCHS = 0\n",
    "LR_EXP_DECAY = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = keras.callbacks.LearningRateScheduler(lrfn(30), verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f696709d2eb6a10088cd2d7ae2ddbbc14796a25c"
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "1e48fdc46842d550348eea153e906331b3db72ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 2 classes.\n",
      "Found 119 images belonging to 2 classes.\n",
      "Found 141 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range=30,width_shift_range=0.1,\\\n",
    "                             height_shift_range=0.1,shear_range=0.1,zoom_range=0.1,\\\n",
    "                                 horizontal_flip=True,vertical_flip=False,validation_split=0.2,\n",
    "                                  preprocessing_function = get_random_eraser(v_l=0, v_h=255))\n",
    "test_datagen = ImageDataGenerator()\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    seed=2019,\n",
    "    color_mode='rgb'\n",
    ")\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, # same directory as training data\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',color_mode='rgb',\n",
    "    subset='validation')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    with strategy.scope():\n",
    "        model = keras.Sequential([\n",
    "                    DenseNet201(input_shape=(224,224, 3),include_top=False,weights='imagenet'),\n",
    "                    keras.layers.GlobalAveragePooling2D(),\n",
    "                    keras.layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        model.compile(\n",
    "                optimizer='adam',\n",
    "                loss = 'categorical_crossentropy',\n",
    "                metrics=['categorical_accuracy']\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "74842112/74836368 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "D_net = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = 600/BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "38/38 [==============================] - 172s 5s/step - loss: 0.8290 - categorical_accuracy: 0.5817 - val_loss: 960.6427 - val_categorical_accuracy: 0.4184\n",
      "Epoch 2/30\n",
      "38/38 [==============================] - 21s 551ms/step - loss: 0.6439 - categorical_accuracy: 0.6400 - val_loss: 27.3813 - val_categorical_accuracy: 0.4113\n",
      "Epoch 3/30\n",
      "38/38 [==============================] - 21s 551ms/step - loss: 0.5777 - categorical_accuracy: 0.7083 - val_loss: 3.8149 - val_categorical_accuracy: 0.5816\n",
      "Epoch 4/30\n",
      "38/38 [==============================] - 22s 571ms/step - loss: 0.6036 - categorical_accuracy: 0.6733 - val_loss: 1.3337 - val_categorical_accuracy: 0.5957\n",
      "Epoch 5/30\n",
      "38/38 [==============================] - 21s 551ms/step - loss: 0.6252 - categorical_accuracy: 0.6700 - val_loss: 42.7393 - val_categorical_accuracy: 0.4184\n",
      "Epoch 6/30\n",
      "38/38 [==============================] - 21s 545ms/step - loss: 0.6647 - categorical_accuracy: 0.6583 - val_loss: 25.2281 - val_categorical_accuracy: 0.5887\n",
      "Epoch 7/30\n",
      "38/38 [==============================] - 22s 572ms/step - loss: 0.5984 - categorical_accuracy: 0.6983 - val_loss: 0.9624 - val_categorical_accuracy: 0.5816\n",
      "Epoch 8/30\n",
      "38/38 [==============================] - 21s 549ms/step - loss: 0.5897 - categorical_accuracy: 0.6883 - val_loss: 1.1011 - val_categorical_accuracy: 0.6241\n",
      "Epoch 9/30\n",
      "38/38 [==============================] - 21s 551ms/step - loss: 0.6271 - categorical_accuracy: 0.6800 - val_loss: 1.1808 - val_categorical_accuracy: 0.6028\n",
      "Epoch 10/30\n",
      "38/38 [==============================] - 22s 574ms/step - loss: 0.5850 - categorical_accuracy: 0.7000 - val_loss: 0.5661 - val_categorical_accuracy: 0.6667\n",
      "Epoch 11/30\n",
      "38/38 [==============================] - 21s 550ms/step - loss: 0.5552 - categorical_accuracy: 0.7283 - val_loss: 0.6289 - val_categorical_accuracy: 0.6879\n",
      "Epoch 12/30\n",
      "38/38 [==============================] - 21s 552ms/step - loss: 0.6072 - categorical_accuracy: 0.6933 - val_loss: 30.6830 - val_categorical_accuracy: 0.5957\n",
      "Epoch 13/30\n",
      "38/38 [==============================] - 21s 557ms/step - loss: 0.6009 - categorical_accuracy: 0.6883 - val_loss: 1.5914 - val_categorical_accuracy: 0.6170\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 14/30\n",
      "38/38 [==============================] - 21s 546ms/step - loss: 0.5698 - categorical_accuracy: 0.7217 - val_loss: 1.2123 - val_categorical_accuracy: 0.6809\n",
      "Epoch 15/30\n",
      "38/38 [==============================] - 21s 559ms/step - loss: 0.5421 - categorical_accuracy: 0.7283 - val_loss: 0.3927 - val_categorical_accuracy: 0.6667\n",
      "Epoch 16/30\n",
      "38/38 [==============================] - 21s 557ms/step - loss: 0.5170 - categorical_accuracy: 0.7583 - val_loss: 0.8929 - val_categorical_accuracy: 0.7234\n",
      "Epoch 17/30\n",
      "38/38 [==============================] - 21s 547ms/step - loss: 0.5067 - categorical_accuracy: 0.7633 - val_loss: 0.4944 - val_categorical_accuracy: 0.6950\n",
      "Epoch 18/30\n",
      "38/38 [==============================] - 22s 567ms/step - loss: 0.4810 - categorical_accuracy: 0.7717 - val_loss: 0.5558 - val_categorical_accuracy: 0.6738\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 19/30\n",
      "38/38 [==============================] - 21s 547ms/step - loss: 0.4789 - categorical_accuracy: 0.7567 - val_loss: 2.0070 - val_categorical_accuracy: 0.6738\n",
      "Epoch 20/30\n",
      "38/38 [==============================] - 21s 546ms/step - loss: 0.4552 - categorical_accuracy: 0.7950 - val_loss: 0.4644 - val_categorical_accuracy: 0.7589\n",
      "Epoch 21/30\n",
      "38/38 [==============================] - 22s 567ms/step - loss: 0.4249 - categorical_accuracy: 0.8050 - val_loss: 0.4720 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 22/30\n",
      "38/38 [==============================] - 21s 542ms/step - loss: 0.3753 - categorical_accuracy: 0.8433 - val_loss: 0.5964 - val_categorical_accuracy: 0.7163\n",
      "Epoch 23/30\n",
      "38/38 [==============================] - 21s 547ms/step - loss: 0.3805 - categorical_accuracy: 0.8450 - val_loss: 0.7212 - val_categorical_accuracy: 0.6809\n",
      "Epoch 24/30\n",
      "38/38 [==============================] - 22s 570ms/step - loss: 0.3813 - categorical_accuracy: 0.8300 - val_loss: 0.4336 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 25/30\n",
      "38/38 [==============================] - 21s 551ms/step - loss: 0.3423 - categorical_accuracy: 0.8500 - val_loss: 0.3704 - val_categorical_accuracy: 0.7801\n",
      "Epoch 26/30\n",
      "38/38 [==============================] - 21s 549ms/step - loss: 0.3226 - categorical_accuracy: 0.8517 - val_loss: 0.7997 - val_categorical_accuracy: 0.7660\n",
      "Epoch 27/30\n",
      "38/38 [==============================] - 22s 572ms/step - loss: 0.3163 - categorical_accuracy: 0.8633 - val_loss: 0.5801 - val_categorical_accuracy: 0.7447\n",
      "Epoch 28/30\n",
      "38/38 [==============================] - 21s 542ms/step - loss: 0.2937 - categorical_accuracy: 0.8800 - val_loss: 0.7700 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 29/30\n",
      "38/38 [==============================] - 21s 553ms/step - loss: 0.2801 - categorical_accuracy: 0.8800 - val_loss: 0.7483 - val_categorical_accuracy: 0.8156\n",
      "Epoch 30/30\n",
      "38/38 [==============================] - 22s 570ms/step - loss: 0.3001 - categorical_accuracy: 0.8633 - val_loss: 0.7462 - val_categorical_accuracy: 0.8085\n"
     ]
    }
   ],
   "source": [
    "history = D_net.fit_generator(train_generator,epochs = 30,\n",
    "            callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = 0.5, patience =3, \n",
    "                                                           min_lr=0.00001, verbose=1, mode='min'),early_stopping],\n",
    "                              validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = D_net.evaluate_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.30673107504844666 test acc: 0.8085106611251831\n"
     ]
    }
   ],
   "source": [
    "print(\"test_loss:\",res[0],\"test acc:\",res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will add, description of code.. soon. Sorry**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
